{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "評価準備\n",
        "\n",
        "### インストール\n",
        "\n",
        "* pip installは失敗することがあるので、失敗したらもう一度実行してください\n",
        "\n",
        "* githubから必要なファイルをコピーしてきます。gitが使える環境が必須です。\n"
      ],
      "metadata": {
        "id": "0JPuqneIxwzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# 1. ライブラリのインストール\n",
        "# 必要なライブラリをインストール\n",
        "#%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -U torch\n",
        "!pip install -U peft\n",
        "\n",
        "!pip install openai\n",
        "!pip install unsloth-zoo # Install or update unsloth-zoo package\n",
        "!pip install --upgrade --no-cache-dir \"unsloth-zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "!pip install --upgrade openai\n",
        "#!pip install --upgrade transformers[olmo2]\n",
        "\n",
        "!pip install openai #==1.55.3 # This line installs openai==1.55.3\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade trl"
      ],
      "metadata": {
        "id": "V26s7DOhQ3fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論＆評価用ライブラリ h.godai\n",
        "!pip install --upgrade git+https://github.com/h-godai/llm"
      ],
      "metadata": {
        "id": "WNGMwrBcu82g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なファイルをgithubから持ってきます。環境構築した直後に一度だけ実行してください。\n",
        "\n",
        "!git clone https://github.com/h-godai/llm.git godai_temp\n",
        "!cp -rv \"godai_temp/LLM2024_env/\" .\n",
        "!rm -r godai_temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqmK068Bm8O2",
        "outputId": "4977da6c-ed16-460e-aa80-221b2430bcd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'godai_temp'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 35 (delta 9), reused 18 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (35/35), 79.07 KiB | 622.00 KiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "'godai_temp/LLM2024_env/LLM2024_Model_Inference_fewshot2_20241217-L4.ipynb' -> './LLM2024_env/LLM2024_Model_Inference_fewshot2_20241217-L4.ipynb'\n",
            "'godai_temp/LLM2024_env/elyza-tasks-100-TV_0.jsonl' -> './LLM2024_env/elyza-tasks-100-TV_0.jsonl'\n",
            "'godai_temp/LLM2024_env/fewshot_prompt.json' -> './LLM2024_env/fewshot_prompt.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "２回目以降はここから実行してください。"
      ],
      "metadata": {
        "id": "iC49iowgtHCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# 各APIキーの取得\n",
        "#OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "#GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# 必要なライブラリを読み込み\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "import inference_evaluator.InferenceEvaluator as FireStorm\n",
        "\n",
        "DataDir = \"./LLM2024_env\""
      ],
      "metadata": {
        "id": "PCY61fguh5ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのロード\n",
        "from huggingface_hub import login\n",
        "from peft import LoraConfig\n",
        "\n",
        "adapter_id = None\n",
        "dpo_adapter_id = None\n",
        "model_id = \"llm-jp/llm-jp-3-13b\"\n",
        "\n",
        "#adapter_id = \"h-godai/llm-jp-3-13b-ft8-cm25k-dpo3_fs2_LoRA\"                # Release 12-14\n",
        "#adapter_id = \"h-godai/llm-jp-3-13b-ft8-cm25k-dpo1.5x2_bad28_3.44_fs2_LoRA\" # Release 15\n",
        "#adapter_id = \"h-godai/llm-jp-3-13b-ft8-cm25k-dpo1.5_1e-04_ep1_x2_bad28-3.44_LoRA\"\n",
        "#adapter_id = \"h-godai/llm-jp-3-13b-ft8-cm25k-dpo2_1e-04_ep1_x1_bad32-3.27_LoRA\" # Rrelease 13\n",
        "adapter_id = \"h-godai/llm-jp-3-13b-ft8-cm25k-dpo1.5x2_bad28_3.44_fs2_LoRA\" # Release 15,19\n",
        "\n",
        "model_name = model_id if adapter_id is None else adapter_id\n",
        "\n",
        "\n",
        "# unslothのFastLanguageModelで元のモデルをロード。\n",
        "dtype = None # Noneにしておけば自動で設定\n",
        "load_in_4bit = True # 今回は13Bモデルを扱うためTrue\n",
        "\n",
        "if adapter_id:\n",
        "  # LoRAと元のモデルを一緒にロード\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name=adapter_id,\n",
        "      dtype=dtype,\n",
        "      load_in_4bit=load_in_4bit,\n",
        "      trust_remote_code=True,\n",
        "  )\n",
        "else:\n",
        "  # 元モデルのみロード\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name=model_id,\n",
        "      dtype=dtype,\n",
        "      load_in_4bit=load_in_4bit,\n",
        "      trust_remote_code=True,\n",
        "  )\n",
        "\n",
        "\n",
        "# 別途DPOがある場合\n",
        "if dpo_adapter_id:\n",
        "  #if adapter_id:\n",
        "  #  model = model.merge_and_unload() # いったんモデルを結合する\n",
        "  # Load the adapter configuration, potentially ignoring unknown keys\n",
        "  config = LoraConfig.from_pretrained(dpo_adapter_id, ignore_mismatched_sizes=True)\n",
        "  model = PeftModel.from_pretrained(model, dpo_adapter_id, config=config)  # Use the loaded config\n",
        "  print(f\"DPO LoRA adapter loaded from {dpo_adapter_id}\")\n",
        "\n",
        "model = FastLanguageModel.for_inference(model) # This line is added to fix the error"
      ],
      "metadata": {
        "id": "lej-Qch7hRqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class FewShotGenerator:\n",
        "\n",
        "  def __init__(self, fewshot_prompt = None):\n",
        "    self.fewshot_prompt = fewshot_prompt\n",
        "\n",
        "  def save(self, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "      json.dump(self.fewshot_prompt, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "  def load(self, path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "      self.fewshot_prompt = json.load(f)\n",
        "\n",
        "  # [{ \"keywords\": [<keyword[:Group]>,...], \"fewshot1\": <fewshot>, \"endshot\": <tailshot> }]\n",
        "  # Group指定がある場合、同一グループのいずれかがマッチすればOK\n",
        "  # それ以外は全マッチが必要\n",
        "  #\n",
        "  def input_prompt_hook(self, eval, prompt):\n",
        "    for fsp in self.fewshot_prompt:\n",
        "      kwlen = len(fsp[\"keywords\"])\n",
        "      ok = True\n",
        "      group = {}\n",
        "      for keyword in fsp[\"keywords\"]:\n",
        "        if ':' in keyword:\n",
        "          # group付、group内のいずれかでOK\n",
        "          words = keyword.split(':')\n",
        "          keyword = words[0]\n",
        "          gr = words[1]\n",
        "          hit = keyword in prompt\n",
        "          if gr not in group:\n",
        "            group[gr] = 0\n",
        "          if hit:\n",
        "            group[gr] += 1\n",
        "        else:\n",
        "          # groupなし。全て一致する必要あり\n",
        "          if keyword not in prompt:\n",
        "            ok = False; # 一つでもなければNG\n",
        "          pass\n",
        "        pass\n",
        "      pass # for keyword in\n",
        "      if ok:\n",
        "        # グループに0があればNG\n",
        "        for gr in group:\n",
        "          if group[gr] == 0:\n",
        "            ok = False;\n",
        "            break\n",
        "          pass\n",
        "        pass\n",
        "      pass\n",
        "      if ok and fsp[\"fewshot1\"] is not None:\n",
        "        if 'endshot' in fsp:\n",
        "          return f\"{fsp['fewshot1']}\\n### 指示\\n{prompt}\\n\\n{fsp['endshot']}\\n### 回答\\n\"\n",
        "        else:\n",
        "          return f\"{fsp['fewshot1']}\\n### 指示\\n{prompt}\\n\\n### 回答\\n\"\n",
        "    pass # for fsp in\n",
        "    # ない場合はデフォルト\n",
        "    return f\"{eval.prefix_prompt_}\\n### 指示\\n{prompt}\\n\\n### 回答\\n\"\n",
        "  pass # def\n",
        "pass  # class\n",
        "\n",
        "fsg = FewShotGenerator()\n",
        "fsg.load(f\"{DataDir}/fewshot_prompt.json\")"
      ],
      "metadata": {
        "id": "VlXe2wEz3tBx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "ここから↓は、LLM2024提出用のJsonl出力\n"
      ],
      "metadata": {
        "id": "MMZ5OlS1iu-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの読み込み。\n",
        "# omnicampusの開発環境では、左にタスクのjsonlをドラッグアンドドロップしてから実行。\n",
        "datasets = []\n",
        "with open(f\"./elyza-tasks-100-TV_0.jsonl\", \"r\") as f:\n",
        "    item = \"\"\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      item += line\n",
        "      if item.endswith(\"}\"):\n",
        "        datasets.append(json.loads(item))\n",
        "        item = \"\""
      ],
      "metadata": {
        "id": "cNznuOp53uPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論開始\n",
        "\n",
        "import inference_evaluator.InferenceEvaluator as FireStorm\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "evaltask = FireStorm.InferenceEvaluator(model, None) # inference only\n",
        "evaltask.tokenizer_ = tokenizer\n",
        "evaltask.max_tokens_ = 1408 # 1024 # max_tokens\n",
        "evaltask.temperature_ = 0.2 # temperature\n",
        "evaltask.repetition_penalty_ = 1.2 # repetition_penalty\n",
        "evaltask.do_sample_ = evaltask.temperature_ > 0\n",
        "evaltask.top_p = 0.9 # top_p\n",
        "\n",
        "evaltask.prefix_prompt_ = \"\" # \"以下の指示に厳密に従って、正確に回答してください。\\n\"\n",
        "evaltask.suffix_prompt_ = \"\"\n",
        "evaltask.input_prompt_hook_ = fsg.input_prompt_hook # few shot hook\n",
        "\n",
        "results = []\n",
        "for data in datasets:\n",
        "  print(f\"TaskId: {evaltask.eval_count_}\")\n",
        "  input = data[\"input\"]\n",
        "  output = evaltask.inference(input)\n",
        "  print(output)\n",
        "  results.append({\"task_id\": data[\"task_id\"], \"input\": input, \"output\": output})\n",
        "  evaltask.eval_count_ += 1\n"
      ],
      "metadata": {
        "id": "vbWe7xU93u5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# こちらで生成されたjsolを提出してください。\n",
        "# 本コードではinputも含んでいますが、なくても問題ありません。\n",
        "# 必須なのはtask_idとoutputとなります。\n",
        "import re\n",
        "import datetime\n",
        "import pytz\n",
        "now = datetime.datetime.now(pytz.timezone(\"Asia/Tokyo\")).strftime('%Y%m%d_%H%M%S')\n",
        "with open(f\"./outputs-{now}.jsonl\", 'w', encoding='utf-8') as f:\n",
        "    for result in results:\n",
        "      print(result)\n",
        "      json.dump(result, f, ensure_ascii=False)  # ensure_ascii=False for handling non-ASCII characters\n",
        "      f.write('\\n')"
      ],
      "metadata": {
        "id": "7gROED3q3xkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELYZA-tasks-100の評価を行います。\n",
        "デフォルトではGeminiのAPIを使います。"
      ],
      "metadata": {
        "id": "2djci2Q77nnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kon4ZKaZgbkN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ELYZA-task-100の評価を行う\n",
        "\n",
        "import inference_evaluator.InferenceEvaluator as FireStorm\n",
        "from datasets import load_dataset\n",
        "\n",
        "if False:\n",
        "  GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "  # ELYZA-tasks-100をダウンロードしてデータセットとして読み込み\n",
        "  elyza100_datasets = load_dataset(\"elyza/ELYZA-tasks-100\")\n",
        "  !mkdir outputs\n",
        "\n",
        "  # 評価実行\n",
        "  #evaltask = InferenceEvaluator(model, OPENAI_API_KEY)\n",
        "  evaltask = FireStorm.InferenceEvaluator(model, GEMINI_API_KEY, \"gemini\") # using GEMINI\n",
        "  evaltask.prefix_prompt_ = \"\" #\"以下の指示に厳密に従って、正確に回答してください。\\n\"\n",
        "  #evaltask.suffix_prompt_ = \"\" # \"簡潔に回答してください\\n\"\n",
        "  evaltask.input_prompt_hook_ = fsg.input_prompt_hook # few shot hook\n",
        "  evaltask.run(dataset = elyza100_datasets['test'],\n",
        "              name = model_name+\"_fs2\",\n",
        "              tokenizer = tokenizer,\n",
        "              temperature = 0.0,\n",
        "              repetition_penalty = 1.2, #1.2,\n",
        "              max_tokens= 1024 # 1408 # 1024 # 1408 # 1024 #1280\n",
        "  )\n",
        "  evaltask.write_result()\n"
      ]
    }
  ]
}