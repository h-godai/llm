{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"88d420bf410c4433a745305b23359fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1eee859f029f4bbcb8d7f8f27eb71e41","IPY_MODEL_d3bed6bb37874872bab40da7749e007a","IPY_MODEL_ed8ad264538341bda9cd418318217b06"],"layout":"IPY_MODEL_9ececca6962a48e79adf9e4a7016311c"}},"1eee859f029f4bbcb8d7f8f27eb71e41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7c45de001be4db193e412500269027e","placeholder":"​","style":"IPY_MODEL_e552aa0e857547ada2d266809ecc1d79","value":"Loading checkpoint shards: 100%"}},"d3bed6bb37874872bab40da7749e007a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10d8689c1cab470484fc2bc2822786e6","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_591987ef7fbb45d697928d2bb342ddad","value":2}},"ed8ad264538341bda9cd418318217b06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f8b5c7003bc4bdab7bb88a490be1744","placeholder":"​","style":"IPY_MODEL_b8bfe00ba3df46d7a850b31afc588fa8","value":" 2/2 [02:39&lt;00:00, 67.11s/it]"}},"9ececca6962a48e79adf9e4a7016311c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7c45de001be4db193e412500269027e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e552aa0e857547ada2d266809ecc1d79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10d8689c1cab470484fc2bc2822786e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"591987ef7fbb45d697928d2bb342ddad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f8b5c7003bc4bdab7bb88a490be1744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8bfe00ba3df46d7a850b31afc588fa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtjqrpmHeGQQ","outputId":"c70bd518-2736-489e-8e82-06a632e20dbe","executionInfo":{"status":"ok","timestamp":1751922308532,"user_tz":-540,"elapsed":1992,"user":{"displayName":"Kohei Ikeda","userId":"06270094982881651260"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.11.12\n","2.7.1+cu128\n","CUDA: True\n","Mon Jul  7 21:05:08 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 575.64.01              Driver Version: 576.80         CUDA Version: 12.9     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA GeForce RTX 5090        On  |   00000000:01:00.0  On |                  N/A |\n","|  0%   43C    P8             36W /  575W |   10601MiB /  32607MiB |      7%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["import torch\n","!python --version\n","print(torch.__version__)\n","print(f\"CUDA: {torch.cuda.is_available()}\")\n","# GPUの情報を確認\n","!nvidia-smi"]},{"cell_type":"code","source":["# google colabなどの環境では、新しいpytorchが必要になります。これらをinstallしてください\n","#!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n","#!pip install -U transformers accelerate bitsandbytes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xL0zsDHFo_N4","executionInfo":{"status":"ok","timestamp":1751921865183,"user_tz":-540,"elapsed":14795,"user":{"displayName":"Kohei Ikeda","userId":"06270094982881651260"}},"outputId":"273622b1-3d32-4754-a4ec-8bb7b260bd53"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n","Collecting transformers\n","  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.1+cu128)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.61)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.57)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.57)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.7.1.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.55)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.61)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n","Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.1)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers, bitsandbytes\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.53.0\n","    Uninstalling transformers-4.53.0:\n","      Successfully uninstalled transformers-4.53.0\n","Successfully installed bitsandbytes-0.46.1 transformers-4.53.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"650ea86872f449099a4b6cde6c9f488e"}},"metadata":{}}]},{"cell_type":"code","source":["\n","# ローカルLLMの例　（HuggingFaceのアカウントが必要です）\n","\n","# --- モデルとトークナイザのロード (4-bit量子化設定) ---\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# モデル名\n","model_name = \"google/gemma-2-2b-jpn-it\"\n","# 4-bit量子化のための設定を作成\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\", # 量子化のタイプ (nf4が推奨されることが多い)\n","    bnb_4bit_compute_dtype=torch.bfloat16, # 計算時のデータ型 (bfloat16が効率的)\n","    bnb_4bit_use_double_quant=True, # 二重量子化を有効にしてメモリをさらに節約\n",")\n","\n","print(\"\\nモデルとトークナイザのロードを開始します (4-bit量子化)...\")\n","# トークナイザのロード\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# モデルのロード(RTX5090で300W)\n","# quantization_config パラメータに上記の設定を渡すことで、モデルが4-bitでロードされます。\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=quantization_config,\n","    device_map=\"auto\", # モデルを自動でGPUに配置\n",")\n","\n","print(\"\\nモデルとトークナイザのロードが完了しました。\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["88d420bf410c4433a745305b23359fc5","1eee859f029f4bbcb8d7f8f27eb71e41","d3bed6bb37874872bab40da7749e007a","ed8ad264538341bda9cd418318217b06","9ececca6962a48e79adf9e4a7016311c","c7c45de001be4db193e412500269027e","e552aa0e857547ada2d266809ecc1d79","10d8689c1cab470484fc2bc2822786e6","591987ef7fbb45d697928d2bb342ddad","6f8b5c7003bc4bdab7bb88a490be1744","b8bfe00ba3df46d7a850b31afc588fa8"]},"id":"EgsYIUB1iWW1","executionInfo":{"status":"ok","timestamp":1751922496868,"user_tz":-540,"elapsed":176180,"user":{"displayName":"Kohei Ikeda","userId":"06270094982881651260"}},"outputId":"efa973ef-77d0-4cdf-c6d0-53ac9d2c9943"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","モデルとトークナイザのロードを開始します (4-bit量子化)...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d420bf410c4433a745305b23359fc5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","モデルとトークナイザのロードが完了しました。\n"]}]},{"cell_type":"code","source":["%%time\n","# テキスト要約のサンプル\n","# google-colab A100(BF16:312TFLOPS): CPU times: user 1min 13s, sys: 752 ms, total: 1min 13s Wall time: 1min 13s\n","#     local RTX5090(BGF): CPU times: user 26.1 s, sys: 4.83 s, total: 30.9 sWall time: 29.7 s\n","\n","# テキスト生成パイプラインの準備 ---\n","# Hugging Faceのpipelineを使うと簡単にテキスト生成ができます。\n","from transformers import pipeline\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n",")\n","\n","print(\"\\nテキスト生成パイプラインの準備が完了しました。\")\n","\n","long_text = \"\"\"\n","深層学習（ディープラーニング）とは何か？\n","\n","深層学習（Deep Learning）は、機械学習（Machine Learning）の一種であり、人工知能（AI）の分野において、\n","現在最も広く活用され、注目を浴びている手法です。\n","機械学習は、「データから規則性やパターンを学習し、新しいデータに対して適切な予測や判断を行う」ための技術を指します。\n","その中でも、深層学習は、特に人間の脳神経細胞（ニューロン）の働きを数理モデル化した「ニューラルネットワーク」を、多層化したものです。\n","ニューラルネットワークの層を深く（deep）積み重ねることで、データに含まれる複雑なパターンや特徴を効果的に抽出・学習できることから、この名前がついています。\n","\n","深層学習の仕組み（ニューラルネットワークの概要）\n","\n","深層学習の根幹となるニューラルネットワークは、人工的に作られた神経細胞（ニューロン）の層が複数積み重なった構造をしています。\n","基本構成は以下の通りです。\n","入力層 (Input Layer)\n","データをニューラルネットワークに取り込む最初の層。画像、音声、テキストなどの生データがここに入ります。\n","中間層（隠れ層：Hidden Layers）\n","入力データから複雑な特徴を抽出し、それを学習する層。深層学習では、この中間層が複数存在することで、「より複雑な表現」を学習できます。\n","出力層 (Output Layer)\n","最終的な結果（分類、回帰予測、生成された画像など）を出力する層。\n","学習時、各ニューロンは重み（weights）とバイアス（biases）というパラメータを持ちます。\n","データが入力されると、このパラメータを使って各ニューロンが計算を行い、活性化関数（activation function）を通して出力を生成します。\n","この出力が次の層に伝播（forward propagation）されます。最終的にネットワークが出力した結果と正解データを比較し、誤差を逆向きに伝播\n","（back propagation）させてパラメータを調整します。これを「誤差逆伝播法（Back Propagation）」と呼びます。\n","\n","深層学習が注目される理由\n","\n","従来の機械学習では、専門家が特徴量を手作業で抽出（特徴エンジニアリング）する必要がありました。\n","例えば、画像認識なら「色や形状を特徴として選ぶ」などです。\n","しかし、深層学習は、データから特徴量を自動で抽出する能力を持ち、手作業の特徴設計が不要になります。\n","これを「表現学習（Representation Learning）」と言います。\n","\n","深層学習が発展した背景には、以下の3つの要素が大きく寄与しています。\n","計算資源の向上（GPU、TPUなどの高速な並列処理ハードウェア）\n","大量かつ高品質なデータセットの整備（画像ならImageNet、テキストならWikipediaなど）\n","アルゴリズムの改善（Dropout、Batch Normalization、Transformerなど新たなアーキテクチャや技術の開発）\n","これらが揃ったことで、深層学習は2010年代以降に急速に発展を遂げました。\n","\"\"\"\n","\n","# --- ステップ5: テキスト生成の実行 ---\n","# gemma-2-itモデルは、特定の形式のプロンプトで性能を発揮します。\n","prompt_template = \"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n","\n","length_tbl = [50,100,200,300,500,1000]\n","\n","for length in length_tbl:\n","  instruction = f\"以下の文を小学5年生でもわかるように、{length}文字程度に要約して文章にしなさい。箇条書きは禁止です。\"\n","  prompt = prompt_template.format(instruction=instruction + f\"\\n\\n{long_text}\")\n","\n","  print(\"\\n--- instruction ---\")\n","  print(instruction)\n","\n","  # テキスト生成の実行 RTX5090で330W\n","  outputs = pipe(\n","      prompt,\n","      max_new_tokens=1024,\n","      do_sample=True,\n","      temperature=0.7,\n","      top_p=0.95,\n","  )\n","\n","  print(\"\\n--- モデルの応答 ---\")\n","  # pipelineの出力はリスト形式で、生成されたテキストは 'generated_text' キーに格納されています\n","  generated_text = outputs[0]['generated_text']\n","\n","  # プロンプト部分を除去して、モデルの応答だけを表示\n","  model_response = generated_text[len(prompt):].strip()\n","  print(model_response)\n","  print(f\"{len(model_response)} 文字\")\n","\n","print(\"\\n--- 処理完了 ---\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAGDNgL3iuLV","executionInfo":{"status":"ok","timestamp":1751922633847,"user_tz":-540,"elapsed":30888,"user":{"displayName":"Kohei Ikeda","userId":"06270094982881651260"}},"outputId":"f3f2c1cd-6e32-4ba0-f7e5-ea4d7310afaf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["\n","テキスト生成パイプラインの準備が完了しました。\n","\n","--- instruction ---\n","以下の文を小学5年生でもわかるように、50文字程度に要約して文章にしなさい。箇条書きは禁止です。\n","\n","--- モデルの応答 ---\n","深層学習とは、人工知能(AI)で、人間の脳の仕組みを参考に、複雑なデータから特徴を自動で学習する技術です。\n","53 文字\n","\n","--- instruction ---\n","以下の文を小学5年生でもわかるように、100文字程度に要約して文章にしなさい。箇条書きは禁止です。\n","\n","--- モデルの応答 ---\n","深層学習とは、人工知能の技術の一つで、人間の脳のような複雑な仕組みを使って、たくさんのデータから、新しいデータに対して適切な予測や判断ができるよ。 \n","これは、たくさんの層で複雑なパターンを学習できる「ニューラルネットワーク」が特徴だよ。\n","ニューラルネットワークは、入力データから複雑な特徴を抽出、最終的に結果を出力する。\n","160 文字\n","\n","--- instruction ---\n","以下の文を小学5年生でもわかるように、200文字程度に要約して文章にしなさい。箇条書きは禁止です。\n","\n","--- モデルの応答 ---\n","深層学習とは、人工知能（AI）で、人間の脳の仕組みを参考に、複雑なパターンや特徴を自動で探す技術です。\n","\n","深層学習は、たくさんの層（隠れ層）を重ねることで、より複雑な情報を見つけ出すことができます。 \n","\n","例えば、画像認識の場合、深層学習は画像の中の色や形を自動で分析して、物体を識別することができます。\n","\n","深層学習が注目される理由は、従来の機械学習では、人間が複雑な特徴を人工的に抽出する必要がありましたが、深層学習は自動で特徴を抽出できるからです。\n","225 文字\n","\n","--- instruction ---\n","以下の文を小学5年生でもわかるように、300文字程度に要約して文章にしなさい。箇条書きは禁止です。\n","\n","--- モデルの応答 ---\n","深層学習とは、人工知能（AI）で、人間脳の仕組みを参考に、複雑なデータからパターンや特徴を自動で学習する技術です。\n","\n","深層学習は、人工的な神経細胞を多層構造にした「ニューラルネットワーク」を使い、たくさんの層でデータを処理します。入力層でデータを受け取り、中間層で複雑な特徴を抽出し、出力層で結果を出力します。\n","\n","深層学習は、従来の機械学習では手作業で特徴量を抽出する必要がありましたが、自動で特徴量を抽出できる点が注目されています。 \n","\n","深層学習が注目される理由は、大量のデータを使って、自動で複雑な特徴を学習できる点が挙げられます。  計算資源の向上、大量かつ高品質なデータセットの整備、アルゴリズムの改善などが、深層学習の急速な発展につながりました。\n","328 文字\n","\n","--- instruction ---\n","以下の文を小学5年生でもわかるように、500文字程度に要約して文章にしなさい。箇条書きは禁止です。\n","\n","--- モデルの応答 ---\n","深層学習は、人工知能（AI）で人気のある技術です。 \n","機械学習とは、データから規則やパターンを見つけ、新しいデータに合う予測や判断ができる技術です。 \n","深層学習は、人間の脳の仕組みを参考に、複雑なデータから特徴やパターンを自動で探す技術です。\n","\n","深層学習は、たくさんの層（隠れ層）で構成され、複雑なデータを理解します。\n","それぞれの層には、入力層、中間層、出力層があります。 \n","入力層は、画像、音声、テキストなどのデータを受け、中間層は、複雑な特徴を抽出し、出力層は、最終的な結果を出力します。\n"," \n","深層学習は、従来の機械学習よりも、自動で複雑な特徴を抽出できる点が魅力です。 \n","これは「表現学習」と呼ばれ、専門家が特徴量を手作業で抽出する必要がなくなり、より効率的です。\n","\n","深層学習が発展したのは、パソコンの性能が向上したり、大量のデータが用意できるようになったからです。\n","さらに、新しいアルゴリズムが開発され、より複雑なデータを理解できるようになりました。\n","429 文字\n","\n","--- instruction ---\n","以下の文を小学5年生でもわかるように、1000文字程度に要約して文章にしなさい。箇条書きは禁止です。\n","\n","--- モデルの応答 ---\n","深層学習とは、人工知能（AI）で、人間の脳のように複雑なパターンを学習できる技術です。 \n","\n","機械学習とは、データから規則性やパターンを見つけ出す技術で、深層学習はその中でも進化したものです。\n","深層学習では、人間脳の仕組みを参考に、たくさんの層で複雑なパターンを学習します。\n","\n","深層学習は、入力されたデータから複雑な特徴を自動で抽出し、最終的に正確な答えを出すことができます。\n","これは、従来の機械学習では専門家が手作業で特徴を選んで、深層学習では自動で抽出できる点が大きな魅力です。\n","\n","深層学習が注目される理由としては、計算能力の向上や大量のデータセットの整備、そして新たな技術の開発が挙げられます。\n","これらの要素が揃ったことで、深層学習は急速に進化し、私たちの生活に大きな影響を与えています。\n","346 文字\n","\n","--- 処理完了 ---\n","CPU times: user 26.1 s, sys: 4.83 s, total: 30.9 s\n","Wall time: 29.7 s\n"]}]},{"cell_type":"code","source":["# --- GPUメモリの解放 ---\n","import gc\n","del model\n","del tokenizer\n","del pipe\n","gc.collect()\n","torch.cuda.empty_cache()\n","print(\"\\nGPUメモリを解放しました。\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gacv0RYCiu1j","executionInfo":{"status":"ok","timestamp":1751920832880,"user_tz":-540,"elapsed":59,"user":{"displayName":"Kohei Ikeda","userId":"06270094982881651260"}},"outputId":"71c5fdf5-72fb-4630-de3c-5ce7b013acad"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","GPUメモリを解放しました。\n"]}]}]}